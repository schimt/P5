{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.spatial import Delaunay\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(filepath: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load data from pickle file. The data is expected to be a dictionary:\n",
    "    {node_identifier: pd.DataFrame with 'traffic_flow', 'Xkoordinat', 'Ykoordinat'}\n",
    "    \"\"\"\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def split_dataframes(df_dict: dict, train_ratio=0.7, val_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits the dictionary of DataFrames into train, validation, and test sets.\n",
    "    Splits are done by time sequence.\n",
    "\n",
    "    Args:\n",
    "        df_dict: {node_key: pd.DataFrame}\n",
    "        train_ratio: fraction of data for training\n",
    "        val_ratio: fraction of data for validation\n",
    "\n",
    "    Returns:\n",
    "        df_dict_train, df_dict_val, df_dict_test\n",
    "    \"\"\"\n",
    "    df_dict_train, df_dict_val, df_dict_test = {}, {}, {}\n",
    "    for key, df in df_dict.items():\n",
    "        n = len(df)\n",
    "        train_end = int(n * train_ratio)\n",
    "        val_end = int(n * (train_ratio + val_ratio))\n",
    "        df_train = df.iloc[:train_end].copy()\n",
    "        df_val = df.iloc[train_end:val_end].copy()\n",
    "        df_test = df.iloc[val_end:].copy()\n",
    "\n",
    "        df_dict_train[key] = df_train\n",
    "        df_dict_val[key] = df_val\n",
    "        df_dict_test[key] = df_test\n",
    "\n",
    "    return df_dict_train, df_dict_val, df_dict_test\n",
    "\n",
    "\n",
    "def create_sliding_window_data(df_dict: dict, lookback: int, pred_horizon: int):\n",
    "    \"\"\"\n",
    "    Create sliding windows for each node.\n",
    "    Returns {node: X_node}, {node: y_node}\n",
    "    X_node shape: (#samples, lookback), y_node shape: (#samples, pred_horizon)\n",
    "    \"\"\"\n",
    "    X_dict, y_dict = {}, {}\n",
    "    for key, df in df_dict.items():\n",
    "        node_series = df['traffic_flow'].values\n",
    "        X_node, y_node = [], []\n",
    "        for i in range(len(node_series) - lookback - pred_horizon + 1):\n",
    "            X_node.append(node_series[i:i+lookback])\n",
    "            y_node.append(node_series[i+lookback:i+lookback+pred_horizon])\n",
    "        X_dict[key] = np.array(X_node)\n",
    "        y_dict[key] = np.array(y_node)\n",
    "    return X_dict, y_dict\n",
    "\n",
    "\n",
    "def combine_node_data(X_dict: dict, y_dict: dict):\n",
    "    \"\"\"\n",
    "    Combine data from all nodes into single arrays.\n",
    "    \"\"\"\n",
    "    X_list = [X_dict[node] for node in X_dict]\n",
    "    y_list = [y_dict[node] for node in y_dict]\n",
    "    X_combined = np.concatenate(X_list, axis=0)\n",
    "    y_combined = np.concatenate(y_list, axis=0)\n",
    "    return X_combined, y_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_graph(graph: nx.Graph) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Clean the graph: remove self-loops, duplicate edges, keep largest component.\n",
    "    \"\"\"\n",
    "    g = graph.copy()\n",
    "    g.remove_edges_from(list(nx.selfloop_edges(g)))\n",
    "\n",
    "    if len(g.nodes) > 0:\n",
    "        largest_component = max(nx.connected_components(g), key=len)\n",
    "        g = g.subgraph(largest_component).copy()\n",
    "\n",
    "    return g\n",
    "\n",
    "# =====================================================\n",
    "#                   Graph Construction\n",
    "# =====================================================\n",
    "\n",
    "def create_distance_graph(df_dict, weighted=False, degree=4):\n",
    "    \"\"\"\n",
    "    Create a graph where edges are based on spatial proximity.\n",
    "    \"\"\"\n",
    "    graph = nx.Graph()\n",
    "    coords = {}\n",
    "    for key, df in df_dict.items():\n",
    "        x, y = df['Xkoordinat'].iloc[0], df['Ykoordinat'].iloc[0]\n",
    "        graph.add_node(key, x=x, y=y, traffic_flow=df['traffic_flow'].values)\n",
    "        coords[key] = (x, y)\n",
    "\n",
    "    # Compute all pairwise distances\n",
    "    keys = list(df_dict.keys())\n",
    "    dist_list = []\n",
    "    for i, k1 in enumerate(keys):\n",
    "        for k2 in keys[i+1:]:\n",
    "            x1, y1 = coords[k1]\n",
    "            x2, y2 = coords[k2]\n",
    "            dist = math.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "            dist_list.append((k1, k2, dist))\n",
    "\n",
    "    # Sort edges by distance (ascending)\n",
    "    dist_list.sort(key=lambda x: x[2])\n",
    "\n",
    "    # Add edges while respecting degree constraints\n",
    "    for k1, k2, dist in dist_list:\n",
    "        if graph.degree[k1] < degree and graph.degree[k2] < degree:\n",
    "            if weighted:\n",
    "                # Normalize weights if desired\n",
    "                graph.add_edge(k1, k2, weight=1/(1+dist))\n",
    "            else:\n",
    "                graph.add_edge(k1, k2)\n",
    "\n",
    "\n",
    "    graph = clean_graph(graph)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_delaunay_graph(df_dict):\n",
    "    \"\"\"\n",
    "    Create a graph based on the Delaunay triangulation of spatial coordinates.\n",
    "\n",
    "    Parameters:\n",
    "        df_dict (dict): A dictionary where keys are node identifiers and values are\n",
    "                        pandas DataFrames containing 'Xkoordinat' and 'Ykoordinat'.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: A Delaunay graph where edges are formed from Delaunay triangulation.\n",
    "    \"\"\"\n",
    "    # Initialize an empty graph\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Extract node coordinates and their identifiers\n",
    "    nodes = []\n",
    "    coords = []\n",
    "    for key, df in df_dict.items():\n",
    "        x, y = df['Xkoordinat'].iloc[0], df['Ykoordinat'].iloc[0]\n",
    "        graph.add_node(key, x=x, y=y, traffic_flow=df['traffic_flow'].values)\n",
    "        nodes.append(key)\n",
    "        coords.append((x, y))\n",
    "\n",
    "    # Perform Delaunay triangulation\n",
    "    tri = Delaunay(coords)\n",
    "\n",
    "    # Add edges from the Delaunay triangulation\n",
    "    for simplex in tri.simplices:\n",
    "        for i in range(3):\n",
    "            for j in range(i + 1, 3):\n",
    "                node1 = nodes[simplex[i]]\n",
    "                node2 = nodes[simplex[j]]\n",
    "                graph.add_edge(node1, node2)\n",
    "\n",
    "    # Clean the graph (remove self-loops, keep largest connected component, etc.)\n",
    "    graph = clean_graph(graph)\n",
    "\n",
    "    return graph\n",
    "\n",
    "def create_correlation_graph(df_dict_train, df_dict_full, threshold=0.5, weighted=False, degree=6):\n",
    "    \"\"\"\n",
    "    Create a correlation-based graph using training data for correlation calculation.\n",
    "    \"\"\"\n",
    "    graph = nx.Graph()\n",
    "    keys = list(df_dict_full.keys())\n",
    "    for key, df in df_dict_full.items():\n",
    "        graph.add_node(key, x=df['Xkoordinat'].iloc[0], y=df['Ykoordinat'].iloc[0],\n",
    "                       traffic_flow=df['traffic_flow'].values)\n",
    "\n",
    "    # Compute correlation\n",
    "    corr_matrix = {}\n",
    "    for i, k1 in enumerate(keys):\n",
    "        for j, k2 in enumerate(keys[i+1:], i+1):\n",
    "            corr = np.corrcoef(df_dict_train[k1]['traffic_flow'], df_dict_train[k2]['traffic_flow'])[0, 1]\n",
    "            corr_matrix[(k1, k2)] = corr\n",
    "            corr_matrix[(k2, k1)] = corr\n",
    "\n",
    "    # Sort edges by correlation\n",
    "    for k1 in keys:\n",
    "        potential = [(k2, corr_matrix[(k1, k2)]) for k2 in keys if k1 != k2 and (k1, k2) in corr_matrix]\n",
    "        potential.sort(key=lambda x: -x[1])  # descending order by correlation\n",
    "        for k2, c in potential:\n",
    "            if graph.degree[k1] < degree and graph.degree[k2] < degree and c > threshold:\n",
    "                if weighted:\n",
    "                    graph.add_edge(k1, k2, weight=c)\n",
    "                else:\n",
    "                    graph.add_edge(k1, k2)\n",
    "    \n",
    "    # find any nodes that are not connected to any other nodes, and connect\n",
    "    graph = clean_graph(graph)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_cosine_graph(df_dict, weighted=False, degree=4):\n",
    "    \"\"\"\n",
    "    Create a graph based on cosine similarity between full node time series.\n",
    "    \"\"\"\n",
    "    graph = nx.Graph()\n",
    "    keys = list(df_dict.keys())\n",
    "    features = [df_dict[k]['traffic_flow'].values for k in keys]\n",
    "    features = np.array(features)  # shape: (#nodes, #timesteps)\n",
    "    # Compute cosine similarity\n",
    "    norm = np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    normed = features / (norm + 1e-8)\n",
    "    sim_matrix = normed @ normed.T  # cosine similarity\n",
    "\n",
    "    for i, k1 in enumerate(keys):\n",
    "        graph.add_node(k1,\n",
    "                       x=df_dict[k1]['Xkoordinat'].iloc[0],\n",
    "                       y=df_dict[k1]['Ykoordinat'].iloc[0],\n",
    "                       traffic_flow=df_dict[k1]['traffic_flow'].values)\n",
    "\n",
    "    # Sort by similarity\n",
    "    for i, k1 in enumerate(keys):\n",
    "        potential = [(keys[j], sim_matrix[i, j]) for j in range(len(keys)) if i != j]\n",
    "        potential.sort(key=lambda x: -x[1])\n",
    "        for k2, sim_val in potential:\n",
    "            if graph.degree[k1] < degree and graph.degree[k2] < degree:\n",
    "                if weighted:\n",
    "                    graph.add_edge(k1, k2, weight=sim_val)\n",
    "                else:\n",
    "                    graph.add_edge(k1, k2)\n",
    "    graph = clean_graph(graph)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_fully_connected_graph(df_dict, weighted=False):\n",
    "    \"\"\"\n",
    "    Create a fully connected graph. Optionally assign random weights.\n",
    "    \"\"\"\n",
    "    graph = nx.Graph()\n",
    "    keys = list(df_dict.keys())\n",
    "    for k in keys:\n",
    "        df = df_dict[k]\n",
    "        graph.add_node(k, x=df['Xkoordinat'].iloc[0], y=df['Ykoordinat'].iloc[0],\n",
    "                       traffic_flow=df['traffic_flow'].values)\n",
    "\n",
    "    for i, k1 in enumerate(keys):\n",
    "        for k2 in keys[i+1:]:\n",
    "            if weighted:\n",
    "                graph.add_edge(k1, k2, weight=np.random.rand())\n",
    "            else:\n",
    "                graph.add_edge(k1, k2)\n",
    "    graph = clean_graph(graph)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def create_dynamic_weight_graph(df_dict, lookback, pred_horizon, adaptive_factor=1.0, max_degree=4):\n",
    "    \"\"\"\n",
    "    Create a graph with dynamic weights based on correlation of sliding windows.\n",
    "    \"\"\"\n",
    "    graph = nx.Graph()\n",
    "    keys = list(df_dict.keys())\n",
    "    for k in keys:\n",
    "        df = df_dict[k]\n",
    "        graph.add_node(k, x=df['Xkoordinat'].iloc[0], y=df['Ykoordinat'].iloc[0],\n",
    "                       traffic_flow=df['traffic_flow'].values)\n",
    "\n",
    "    # Precompute sliding windows\n",
    "    sliding_windows = {\n",
    "        k: np.lib.stride_tricks.sliding_window_view(df_dict[k]['traffic_flow'].values, lookback)[:-pred_horizon]\n",
    "        for k in keys\n",
    "    }\n",
    "\n",
    "    # Compute avg correlation over time for each pair\n",
    "    for i, k1 in enumerate(keys):\n",
    "        for k2 in keys[i+1:]:\n",
    "            w1 = sliding_windows[k1]\n",
    "            w2 = sliding_windows[k2]\n",
    "            # correlation per window\n",
    "            corrs = []\n",
    "            for t in range(min(len(w1), len(w2))):\n",
    "                if np.std(w1[t]) > 0 and np.std(w2[t]) > 0:\n",
    "                    c = np.corrcoef(w1[t], w2[t])[0, 1]\n",
    "                    corrs.append(c)\n",
    "            if len(corrs) > 0:\n",
    "                avg_weight = np.mean(corrs)\n",
    "                threshold = adaptive_factor * avg_weight\n",
    "                if avg_weight > threshold:\n",
    "                    graph.add_edge(k1, k2, weight=avg_weight)\n",
    "\n",
    "    # Enforce max degree\n",
    "    for node in list(graph.nodes()):\n",
    "        edges = sorted(graph.edges(node, data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "        if len(edges) > max_degree:\n",
    "            for edge in edges[max_degree:]:\n",
    "                graph.remove_edge(edge[0], edge[1])\n",
    "\n",
    "    graph = clean_graph(graph)\n",
    "    return graph\n",
    "\n",
    "\n",
    "def TOTAL_RANDOM_WEIGHTS(df_dict, degrees=32):\n",
    "    # Initialize a graph\n",
    "    num_nodes = len(df_dict)\n",
    "    graph = nx.Graph()\n",
    "\n",
    "    # Add nodes with attributes\n",
    "    for i, (key, df) in enumerate(df_dict.items()):\n",
    "        graph.add_node(i, x=df['Xkoordinat'].iloc[0], y=df['Ykoordinat'].iloc[0], traffic_flow=df['traffic_flow'].values)\n",
    "\n",
    "    # Randomly add edges while respecting degree constraints\n",
    "    nodes = list(graph.nodes)\n",
    "    max_attempts = num_nodes * degrees * 10  # Safety limit to prevent infinite loops\n",
    "    attempts = 0\n",
    "\n",
    "    while any(graph.degree(n) < degrees for n in nodes):\n",
    "        u, v = np.random.choice(nodes, size=2, replace=False)\n",
    "        # Add an edge only if it doesn't exist and both nodes are under degree limit\n",
    "        if not graph.has_edge(u, v) and graph.degree(u) < degrees and graph.degree(v) < degrees:\n",
    "            graph.add_edge(u, v, weight=np.random.rand())\n",
    "        \n",
    "        attempts += 1\n",
    "        if attempts >= max_attempts:\n",
    "            print(\"Warning: Maximum attempts reached, stopping early.\")\n",
    "            break\n",
    "\n",
    "    # Return the graph and edge weights\n",
    "    return graph, nx.get_edge_attributes(graph, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_prep/one_year.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 98\u001b[0m\n\u001b[0;32m     94\u001b[0m VAL_RATIO \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Load and split data\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m final_dataframes \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m df_dict_train, df_dict_val, df_dict_test \u001b[38;5;241m=\u001b[39m split_dataframes(final_dataframes, TRAIN_RATIO, VAL_RATIO)\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# Create sliding window data\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(filepath: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    Load data from pickle file. The data is expected to be a dictionary:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m    {node_identifier: pd.DataFrame with 'traffic_flow', 'Xkoordinat', 'Ykoordinat'}\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m         data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\gonde\\anaconda3\\envs\\pyhope\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_prep/one_year.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, latent_channels):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_channels * 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, z, edge_index):\n",
    "        edge_embeddings = torch.cat([z[edge_index[0]], z[edge_index[1]]], dim=1)\n",
    "        return torch.sigmoid(self.fc2(F.relu(self.fc1(edge_embeddings))))\n",
    "\n",
    "    \n",
    "class GraphAutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Autoencoder with GCN-based encoder and inner product decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, hidden_channels, latent_channels):\n",
    "        super(GraphAutoEncoder, self).__init__()\n",
    "        # Encoder: GCN layers\n",
    "        self.encoder1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.encoder2 = GCNConv(hidden_channels, latent_channels)\n",
    "        # Decoder: Inner product decoder\n",
    "        self.decoder = MLPDecoder(latent_channels)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # Encoder: Node feature transformations\n",
    "        x = F.relu(self.encoder1(x, edge_index))\n",
    "        z = self.encoder2(x, edge_index)\n",
    "        # Decoder: Reconstruct probabilities for given edges\n",
    "        recon_adj = self.decoder(z, edge_index)\n",
    "        return z, recon_adj\n",
    "\n",
    "# Updated loss function\n",
    "def gae_loss(recon_adj, edge_index, data):\n",
    "    \"\"\"\n",
    "    Reconstruction loss: Compare reconstructed probabilities with actual edges.\n",
    "    \"\"\"\n",
    "    # Positive edges (existing edges in the graph)\n",
    "    pos_mask = torch.ones_like(recon_adj)\n",
    "    pos_loss = -torch.log(recon_adj + 1e-15).mean()\n",
    "\n",
    "    # Negative edges\n",
    "    num_nodes = data.num_nodes\n",
    "    neg_edge_index = torch.randint(0, num_nodes, edge_index.size(), dtype=torch.long, device=edge_index.device)\n",
    "    neg_loss = -torch.log(1 - recon_adj + 1e-15).mean()\n",
    "\n",
    "    return pos_loss + neg_loss\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_gae(model, data, epochs, lr, device):\n",
    "    \"\"\"\n",
    "    Train the Graph Autoencoder model.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        z, recon_adj = model(data)\n",
    "        loss = gae_loss(recon_adj, data.edge_index, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def evaluate_gae(model, data):\n",
    "    \"\"\"\n",
    "    Evaluate the Graph Autoencoder: Compute reconstruction metrics.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z, recon_adj = model(data)\n",
    "        # Convert reconstructed adjacency to binary predictions\n",
    "        pred_adj = (recon_adj > 0.5).float()\n",
    "        actual_adj = torch.zeros_like(pred_adj)\n",
    "        actual_adj[data.edge_index[0], data.edge_index[1]] = 1.0\n",
    "        # Compute metrics: Reconstruction accuracy\n",
    "        correct = (pred_adj == actual_adj).sum().item()\n",
    "        total = pred_adj.numel()\n",
    "        accuracy = correct / total\n",
    "        print(f\"Reconstruction Accuracy: {accuracy:.4f}\")\n",
    "        return accuracy, z\n",
    "\n",
    "\n",
    "DATA_PATH = 'data_prep/one_year.pkl'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LOOKBACK = 50\n",
    "PRED_HORIZON = 1\n",
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "\n",
    "# Load and split data\n",
    "final_dataframes = load_data(DATA_PATH)\n",
    "df_dict_train, df_dict_val, df_dict_test = split_dataframes(final_dataframes, TRAIN_RATIO, VAL_RATIO)\n",
    "\n",
    "# Create sliding window data\n",
    "X_train_dict, y_train_dict = create_sliding_window_data(df_dict_train, LOOKBACK, PRED_HORIZON)\n",
    "X_val_dict, y_val_dict = create_sliding_window_data(df_dict_val, LOOKBACK, PRED_HORIZON)\n",
    "X_test_dict, y_test_dict = create_sliding_window_data(df_dict_test, LOOKBACK, PRED_HORIZON)\n",
    "\n",
    "node_lengths = {node: X_train_dict[node].shape[0] for node in X_train_dict}\n",
    "\n",
    "# Combine data\n",
    "X_train, y_train = combine_node_data(X_train_dict, y_train_dict)\n",
    "X_val, y_val = combine_node_data(X_val_dict, y_val_dict)\n",
    "X_test, y_test = combine_node_data(X_test_dict, y_test_dict)\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float, device=device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float, device=device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float, device=device)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float, device=device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float, device=device)\n",
    "# Example Usage\n",
    "\n",
    "# Example Usage\n",
    "DEGREE = 6\n",
    "HIDDEN_CHANNELS = 32\n",
    "in_channels = LOOKBACK  # Input features (sliding window size)\n",
    "hidden_channels = HIDDEN_CHANNELS  # Hidden layer size\n",
    "latent_channels = 16  # Latent space dimension\n",
    "gae_model = GraphAutoEncoder(in_channels, hidden_channels, latent_channels)\n",
    "\n",
    "# Prepare graph data\n",
    "graph = create_distance_graph(final_dataframes, weighted=True, degree=DEGREE)\n",
    "node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "edge_index = torch.tensor(\n",
    "    [[node_mapping[u], node_mapping[v]] for u, v in graph.edges()],\n",
    "    dtype=torch.long, device=device\n",
    ").t().contiguous()\n",
    "gae_data = Data(x=X_train_tensor, edge_index=edge_index, num_nodes=len(graph.nodes))\n",
    "\n",
    "# Train GE\n",
    "gae_model = train_gae(gae_model, gae_data, epochs=500, lr=1e-4, device=device)\n",
    "\n",
    "# Evaluate GAE\n",
    "accuracy, embeddings = evaluate_gae(gae_model, gae_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
